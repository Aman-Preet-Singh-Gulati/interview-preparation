{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Learn Machine Learning Algorithms For Interviews\n",
    "\n",
    "#### Naive Bayes Classifier\n",
    "\n",
    "Theoretical Understanding:\n",
    "\n",
    "1. Tutorial 48th : https://www.youtube.com/watch?v=jS1CKhALUBQ\n",
    "2. Tutorial 49th:  https://www.youtube.com/watch?v=temQ8mHpe3k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We as data science and machine learning enthusiast have learned about various algorithms like Logistic Regression, Linear Regression, Decision Trees, Naive Bayes etc. **But at the same time are we preparing for the interviews?** As we know the end goal is to land our dream job for the companies we are aiming for. Henceforth, knowing how the questions are turned and twisted by interviewer is very much important in order to answer in the most effecient way. For that reason I'm starting with the series of **Top 10 most frequently asked interview questions on various machine learning algorithms**.\n",
    "\n",
    "In this article, we will be covering up the **top 10 interview questions on Naive Bayes classifier** but we are not gonna jump straight over those tricky questions, instead let's first have some high level understanding on this algorithm so that one will be able to understand the concept behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n",
    "\n",
    "\n",
    "Naive bayes is considered to be top choices while dealing with **classification problems** and it has it's roots in the concept of probabilities and more specifically this algorithm is the by-product of **Bayes Theorem**. But you must be thinking that if it is based on Bayes theorem then why this Naive term is in the prefix position as **\"Naive\"** means **\"Dumb\"**. So is this algorithm really dumb or usefull?.\n",
    "\n",
    "Answer is simple and pretty straightforward, This algorithm is not at all Naive but at times quite useful and simple when compare to other complex algorithms. The reason it is known to be the naive bayes is because of it's general assumptions, that takes us to our vry first interview question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are the basic assumption in the case of Naive Bayes classifier?\n",
    "\n",
    "If one wants to give the short answer then they can simply say - **\"Features are independent\"**. But this will not be sufficient hence we need to explain the answer briefly: In Naive Bayes it assumes beforehand that all the features are independent to each other and it treats all of them **seperately** which give each feature an **equal contribution** to the final result. This assumption is known as **I.I.D assumption**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. What are the possible advantages of choosing Naive Bayes classifier?\n",
    "\n",
    "1. As it works independently with each features so we can use it with **large dataset** as well for making **generalised model**.\n",
    "2. It has very much **less sensitive** to other features i.e. it is not much affected from other components because of it's **Naive** nature.\n",
    "3. It has the tendency to work efficiently with both **continous and discrete type** of dataset not only that it is well versed in handling **categorical features** in data. \n",
    "4. When we we have dataset with very **less training data** then we can call up **Naive Bayes** classifier in this sceanrio it outperforms other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What disadvantages of Naive Bayes can make you remove it from your analysis?\n",
    "\n",
    "1. As we say that there is always a two side of the coin, the advantage of naive bayes can also be the disadvantage at some stages. As, it treat all the **predictors independently** for that reason we are not able to use it in all the **real-world cases**. \n",
    "2. This algorithm face a very major problem named as, **\"Zero Frequency problem\"** in which it assign the **zero probabilities** to all the **categorical variables** whose categories were not present in training dataset that introduce lot of **bias** in the model.\n",
    "3. As, the features are **highly correlated** it affects the model performance negatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Is feature scaling required in Naive Bayes?\n",
    "\n",
    "A sure short answer should be: As Naive Bayes classifer is **not dependent on the distance** but on the **probability** hence for that reason feature scaling is not required i.e Any algorithm which is not dependent on distance will not require **feature scaling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Impact of missing values on naive bayes?\n",
    "\n",
    "\n",
    "Naive bayes is one of those algorithm that can handle the missing data at its end only the reason is that in this algo all the attributes are handled seperately during both **model construction** and **prediction time**, If data points are missing for a certain feature than it can be ignored when a probability is calculated for a **seperate class**, that makes it handle the missing data at **model building phase** itself.\n",
    "\n",
    "Do refer to this amazing tutorial for better understanding :https://www.youtube.com/watch?v=EqjyLfpv5oA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Impact of outliers?\n",
    "\n",
    "Naive bayes is **highly impacted with outliers** and completely robust in this case (depending on the USE case we are working on). The reason being, NB classifer assign the **0 probability** for all the data instance that it has not seen in the **training set** which create an issue during the **prediction time** and same goes with outliers also, as it would have been same data that classifier have not seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What are different problem statement you can solve using Naive Bayes?\n",
    "\n",
    "Naive bayes is probabilistic based machine learning algorithm and it can be used widely in many classification tasks like:\n",
    "1. Sentiment Analysis\n",
    "2. Spam classification\n",
    "3. Twitter sentiment analysis\n",
    "4. Document categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Naive bayes falls under category of discriminative or generative classifier?\n",
    "\n",
    "**Straightforward answer is:** Naive Bayes is a generative type of classifier. But this information is not enough we should also know what is generative type of classifier.\n",
    "\n",
    "**Generative:** This type of classifier learns from the model that generates the data behind the scene by estimating the **distribution of the model**. Then it predict the unseen data. Henceforth, same goes for NB classifer as it learns from the distribution of data and don't have the create **decision boumdary** to classify components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. What do you know about posterior and prior probability in Naive Bayes\n",
    "\n",
    "\n",
    "**Prior probability:** This can also be tagged as **initial probability**. It's the part of the Bayesian statistics where it is the probability when the data is not even collected that's why it is known as **\"Prior\"** probability. This probability is the outcome vs the current predictor **before the experiment is performed**.\n",
    "\n",
    "**Posterior probability:** In simple words, this is the probability that we get after **few trials of experiment** it is the ascendant of prior probability, for that reason it is also known as **updated probability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. How Naive Bayes treats categorical and numerical values?\n",
    "\n",
    "For both categorical and numerical values we have **two seperate and dedicated distribution** to deal woth either type of values they are mentioned below:\n",
    " \n",
    "1. **Categorical values:** In this case we can get the probability for categorical variables by using **Multinomial or Bernoulli Distribution**.\n",
    "2. **Numerical values:** While in this situation we can estimate the probability by using **Normal or Gaussian** distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So we are in the last section of this article, and we have reached here after completing top 10 interview questions on **NB classifier**. This segment is usually to discuss everything in brief so that we can list down our learnings in a nutshell.\n",
    "\n",
    "1. Firstly we started off this small journey by introducing the concept behind Naive Bayes algorithm and straight after that we discussed about the **assumptions**, **advantages** and **disadvantages** of **Naive Bayes** classfier.\n",
    "\n",
    "2. Then we move on the some tricky questions like, will this algorithm affected by **outliers** and **missing values**? or will the **feature scaling** is the required step while analysing this classifier.\n",
    "\n",
    "3. At the last we covered some more questions based on mathematical intuition behind this algorithm, like How naive bayes treats **categorical and numerical** values, what is **posterior and prior** probability and at the last whether NB classifier is under **generative or discriminative** category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Support vector machine** is one of the most famous and decorated machine learning algorithm in **classification** problems. The heart and soul of this algorithm is the concept of **Hyperplanes** where these planes help to **categorised** the **high dimensional data** which are either **linearly seperable** or not and that is the USB of this algorithm that it can deal with real world complex situation where most of the time data is non-linear and there comes the role of **SVM kernels**.\n",
    "\n",
    "By far you have got the importance of SVM algorithm though in most of the interviews candidates do focus on **tree-based** algorithm but SVM have some serious use case in real-world which make it one of the **hot topics for interviewrs** to ask for. Hence, in this article we are gonna discuss **10 most asked interview questions on SVM** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. In what conditions you will choose SVM over any other algorithm?\n",
    "\n",
    "There are many reasons why ML engineer will prefer SVM over other algorithms some are mentioned below:\n",
    "\n",
    "1. SVM outperforms any other competitor when it comes to dealing with **high dimensional data** along with **PCA** (helps in reducing the less important feature from the sample space) SVM on other side tends to **increase the dimensions** in order to categorise the related features.\n",
    "\n",
    "2. When it comes to **dealing with Non-linear** dataset then SVM should be the first choice as **linear seperable** data is easy to handle but in real world we will always encounter **non-linear dataset** so, in that case SVM's quality of converting the data into **higher dimensions** works.\n",
    "\n",
    "3. Along with being the supervised machine learning algorithm, SVM have put it's foot forward for **unstructured data** and **semi-structured** like images, text and videos. \n",
    "\n",
    "4. The SVM kernels are the real strength of SVM, sometimes in order to deal with complex problems we need to go for higher dimensionality which tends to have **complex mathematical calculations** so to perform smooth calculations **SVM kernels** plays vital role as it provides certain coefficients values like **gamma** and **Cost-C**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What are the drawbacks of using SVM for classification tasks\n",
    "\n",
    "\n",
    "1. One of the most encountered drawback of this algorithm is that it takes a **lot of training time** as soon as we start feeding the **larger dataset** during model development phase.\n",
    "\n",
    "2. It i always difficult to choose a **good kernel function** because we are looking for that **optimal coefficient value** that will perform with better accuracy, at the same time it will be a bad idea to choose **hit and trial** method as most of them take soo much time in **model training**.\n",
    "\n",
    "3. Hyper parameter tuning in SVM is pretty complex to deal with because of the two main parameters i.e. **Cost-C and Gamma** as it is not a cup of tea to **fine tune** these **hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Is feature scaling a required step while working with SVM?\n",
    "\n",
    "Yes, **feature scaling** is very much **important** step to be followed while we are solving our problems using SVM as **feature scaling** (Standardization or Normalization) is required in every algorithm where **distances are considered** between the observations.\n",
    "\n",
    "In SVM as well we are aiming to **maximize the margin** so that we will have a better accuracy directly or indirectly distances are involved hence, feature scaling is also required.\n",
    "\n",
    "\n",
    "\n",
    "## 4. How SVM is impacted when dataset have missing values?\n",
    "\n",
    "SVM's are normally considered to be ideal choice in terms of constructing the model for classification but at the same time it cannot deal with missing data infact it is quite **sensitive to missing data** which lead to **bad model acccuracy** on testing data. That's why it is recommended that we should deal with all the missing values either during **data cleaning** or **feature engineering** phase.\n",
    "\n",
    "\n",
    "## 5. What are the possible outcomes if outliers are present in the dataset?\n",
    "\n",
    "Being quite popular in classification genre SVM have one major drawback i.e. it is very much **sensitive to outliers** it is because of the **penalty** that each misclassifcation returns which we know as **hinge or convex loss** but these loss don't make it sensitive to outliers it's the **outboundness of convex loss** that makes SVM sensitive to outliers.\n",
    "Read this for in-depth knowledge\n",
    "https://arxiv.org/abs/1409.0934#:~:text=Despite%20its%20popularity%2C%20SVM%20has,causes%20the%20sensitivity%20to%20outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. How one can avoid the condition of overfittiing in SVM?\n",
    "\n",
    "\n",
    "Before moving forward with this answer we need to understand 2 terminologies in SVM:\n",
    "\n",
    "1. **Soft margin:** In this case SVM is not that rigid on the classification i.e. it is not that **strict on training data** to classfiy each data point correctly.\n",
    "\n",
    "2. **Hard margin:** Just opposite of soft margin (obviously!) here it is extreme towards making every classfication to be right in terms of training data sample and **very rigid** too.\n",
    "\n",
    "So, now let's come to our question, we all are well aware of the fact that if we will become very much dependent on the training data sample's accuracy then it will lead to overfitting condition while to make a generalised model we need something that is elastic to both training and testing data hence, **soft margin should be ideal choice to avoid overfitting condition**. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. How kernel trick is helpful in SVM ?\n",
    "\n",
    "When we are choosing SVM for solving **classification problem** our main goal is to attain the **maximum margin** i.e. the **decision boundary** between the classes that are to be seprated but this activity in real world setup is quite complex because there we are dealing with **non-linear** data most of the time so to save us from this situation **SVM kernel** can to rescue us which use some functions like polynomial and Sigmoid to convert the dataset from **lower dimension to higher dimension** which later makes it easy to seperate the complex dataset.\n",
    "\n",
    "There are many kernel function available for usage I'm listing down some popular one here:\n",
    "\n",
    "1. **Linear**\n",
    "2. **Non-Linear**\n",
    "3. **Polynomial**\n",
    "4. **RBF - Radial basis function**\n",
    "5. **Sigmoid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. How SVM is different from K-Nearest Neighbors (KNN)? \n",
    "\n",
    "\n",
    "1.  The very first difference between both of them is, where **Linear SVM is parametric** because of it's nature of producing linear boundaries while, **KNN is is non-parameterized** as it ignores the **priori assumptions** about the **structure** of **class boundary** that in turn makes it flexible enough to deal with **non-collinear** boundaries.\n",
    "\n",
    "2. SVM has the upper hand in terms variance as **KNN has higher variance** than **linear SVM** but KNN can adapts to any **classification boundary** even when the class boundary is unknown. As the training data increases KNN reach to the capability where it reaches **optimal classification** boundary.\n",
    "\n",
    "3. KNN does not have any structure for the class boundaries so the classes created by KNN are **less interpretable** if we compare with **linear SVM**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Which among SVM and Logisitic regression is the better algorithm in handling outliers?\n",
    "\n",
    "\n",
    "When it comes to logistic regression then outliers could have a **bigger impact on the model** as the estimation factors that are involved in logisitic regression **coeffcients** are more likely to be sensitive with outliers.\n",
    "\n",
    "On the other hand, outliers when introduced in **SVM model** then it can shift the **position of hyperplane** but not like that much as it can do in the case of logistic regression also in SVM we can deal with outliers by introducing slack variables. \n",
    "\n",
    "Conclusively, **SVM handles outlier more effeciently** than Logistic Regression though both of them are effected by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So, by far we have discussed **top 10 most asked interview questions on SVM** in depth. I kept the discussion as in depth as possible so the one who is reading this article can also answer any **related cross questions** from the interviewer as well. In this section of the article we are gonna discuss everything we learned in a nutshell.\n",
    "\n",
    "1. First we started with some basic question like **advantages and drawbacks** of SVM algorithm but keeeping the questions **twisted** so that one can be comfortable with twisted yet basic questions asked from interviewer. Then we move to feature engineering related stuffs where we found out that SVM is very much **sensitive to outliers and missing values**.\n",
    "\n",
    "2. Then we levelled up where we saw how SVM can avoid headache situations like **overfititng**. Later we gone through the importance of **SVM kernels** in complex **non-linear** dataset.\n",
    "\n",
    "3. At the last we jumped to **comparison based interview questions** where first we saw How **SVM** differ with **KNN algorithm** related to what is happening in the background. At the last We saw that when compared with **logisiic regression**,  **SVM** seems better in handling outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
